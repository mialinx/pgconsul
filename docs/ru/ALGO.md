# Алгоритм работы PgConsul

Далее в тексте под терминами мастер и реплика подразумеваются процессы PgConsul, работающие на хостах с мастером Postgres и соответственно репликой.

## Общий сценарий работы

PgConsul выполняет основную работу в цикле, вызывая функцию `run_iteration`.
В начале итерации PgConsul делает следующие шаги:

* определяет текущую `role` экземпляра Postgres на своем хосте
* загружает состояние кластера из базы (`db_state`)
* загружает состояние кластера из ZK (`zk_state`)
* захватывает `alive_lock` в ZK для своего хоста (`_zk_alive_refresh`)
* проверяет состояние `maintenance`

Далее, в зависимости от текущий роли Postgres и настроек, выполняется одна из функций:

* `primary_iter` - мастер в HA кластере (именно состояние Postgres, а не обладание `leader_lock`-ом)
* `single_node_primary_iter` - мастер в non-HA (как правило 1-node) кластере
* `replica_iter` - реплика в HA кластере
* `non_ha_replica_iter` - каскадная реплика
* `dead_iter` - не удалось получить текущую роль, как правило в случае если Postgres не доступен

### Сценарий работы мастера (`primary_iter`)

#### Захват `leader_lock`

У нормально работающего кластера мастер должен удерживать `leader_lock`.

Поэтому в первую очередь PgConsul выясняет нужно ли отпустить лок (и вернуться репликой), это может быть в случае:

* мастер должен стать каскадной репликой (в настройках появилось `stream_from`)
* мастер пытался сделать `rewind` и не смог (странно что это в `primary_iter`)
* другой хост уже промоутится (`current_promoting_host` установлен в ZK)

Либо захватить лок, если его никто не держит и таймалйн Postgres совпадает с записанным в ZK, те если нет более свежего мастера. Это может быть например при переподключениях / рестартах PgConsul.

Захватывает `leader_lock`, если этого не удалось - возвращается в кластер репликой.

Создает / удаляет необходимы слоты репликации (странно что так рано?).

Сохраняет текущее состояние `db_state` в ZK (будет `zk_state` на след. итерации)

Обрабатывает незавершенный failover/switchover: если мастером должен был стать текущий хост - просто чистит данные в ZK, иначе возвращается репликой в кластер.

#### Починка проблем

К этому моменту понятно, что текущий хост является легитимным мастером.
Поэтому PgConsul исправляет оставшиеся проблемы, приводя кластер к "правильному" состоянию для данного мастера.

Запускает Pooler, если он не запущен.

Включает архивацию WAL, если она была отключена.

#### Смена типа репликации

PgConsul управляет используемым типом репликации (`async`/`sync`/`quorum`), при необходимости понижая его до `async`. Это нужно для того что бы мастер оставался доступным при отказе реплики (деградация 2=>1 хост).

PgConsul поддерживает два варианта работы, реализованных в виде классов `QuorumReplicationManager` (предпочтительно) и `SingleSyncReplicationManager` (устаревший вариант). Далее логика описана для `QuorumReplicationManager`.

PgConsul вычисляет список живых HA реплик:
* тех, которые удерживают `alive_lock`
* действительно реплицируют, те видны в `pg_stat_replication` как `sync/quorum` (`_get_needed_replication_type`)

Если это число:
* `> 0` - в `synchronous_standby_names` записываются только тех, кто действительно реплицирует.
* `= 0` - переключает Postgres на `async` репликацию

#### Проверка необходимости switchover

Проверяет, что в ZK был выставлен флаг планового переключения и делает switchover (`_do_primary_switchover`)

### Сценарий работы HA реплики (`replica_iter`)

В отсутствии подключения к ZK - не делает ничего.

Записывает текущее состояние в ZK:
* добавляется / удаляется из списка HA хостов
* обновляет информацию о wal_receiver
* обновляет информацию о своих репликах

Проверяет, что в ZK был выставлен флаг планового переключения и делает switchover (`_accept_switchover`)

Если никто не держит `leader_lock` - инициирует процедуру failover (`_accept_failover`)

Если текущий источник репликации отличается от актуального - реплика поворачивается на новый мастер (того кто удерживает `leader_lock`)

Если репликация по каким-то причинам не работает - реплика покидает кворум.
Это важно для 2х-ногих кластеров, чтобы отстающая реплика не оказалась единственным кандидатом (и победителем) в выборах мастера. Далее запускается процедура возврата в кластер (`replica_return`/`_return_to_cluster`)

Если все ок:
* реплика открывает доступ к хосту для клиента
* возвращается в кворум
* настраивает слоты для каскадной репликации

### Сценарий работы сломанного (dead_iter)

В этой ситуации PgConsul в первую очередь:
* закрывается от нагрузки (останавливает Pooler)
* выходит из кворума
* отпускает `leader_lock`, если держал его до этого
